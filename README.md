## Degree project in Machine Learning (Master's of Science) at KTH Royal Institute of Technology 

Transformer is a network architecture that is based on attention mechanism,
dispensing entirely with recurrence and convolutions. It has been shown to outperform
other models based on recurrent or convolutional neural networks. Meanwhile, language agnostic 
sentence embedding models are machine learning models that encode text from different languages into 
a shared embedding space. They can be applied to many downstream tasks  such  as  text  classification, 
clustering while also leverage semantic information for language understanding. 

The latest language agnostic sentence embedding models that are based on Transformer have achieved 
state-of-the-art results in many natural processing tasks and are helping deep learning to 
achieve similar success as it has done within the field of computer vision.

This thesis aims to help Sinch to investigate the possibilities of using the state-of-the-art sentence
embedding models to classify multi-label text messages with missing labels. In order to swiftly get started
with the project, I started to experiment with the dataset from the [Kaggle
Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).
This is a dataset with very similar characteristics as the dataset from Sinch. Because of privacy reasons,
the code for the Sinch dataset will not be published. The experiments related to the Kaggle public datasets
are on the other hand published here.

This repository is in developement. 